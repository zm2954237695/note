##### HashMap的结构

Hashmap的底层是哈希表，而在Java中哈希表是用数组+链表实现的。每一个数组的节点称为bucket桶。但桶中有多个元素时就以链表的方式进行存储。当添加元素时，通过key的hash算法计算出hash值。然后通过hash值(对数组长度取模)映射出存在哪个桶里。如果桶中没有元素则添加成功。如果有元素就代表着散列冲突。比较已经存在的元素的hashCode和新插入的元素的hashCode是否相同，如果没有相同的则添加成功(情况1).如果某个元素的hashCode和插入元素的hashCode相同,则调用key的equals方法继续比较，如果为true则新的值覆盖已经存在的值,否则就插入链表中(情况2).对于情况1和情况2，在jdk1.7中是将新的元素添加到原来的头节点(头插法)。jdk1.8是将新的元素添加到链表末尾节点(尾插法).在jdk1.8中做了一个优化，当桶中元素(链表长度)大于8且数组长度>64则转化为红黑树，如果需要扩容再进行扩容。数组长度小于64会先进行扩容。

jdk1.7及之前: （Entry)数组+链表

jdk1.8：(Node)数组+链表/红黑树

**jdk1.8 put方法**

1. 判断Node数组是否为null或者数组长度是否为0，如果是进行扩容。
2. 根据键值key计算hash值得到在数组中的索引i，如果table[i]==null,直接新建节点插入，转到6.如果不为空，则转向3.
3. 判断table[i]的首个元素是否和key一样,如果相同直接覆盖，否则转向4.这里的相同是指hashcode和equals。
4. 判断table[i]的类型，如果是红黑树，则在红黑树中插入键值对，否则转向5
5. 遍历table[i],判断链表长度是否大于8，如果大于8并且数组的容量大于等于64，就把链表转化为红黑树，在红黑树中执行插入操作，否则执行链表的插入的操作;遍历过程中如果发现key已经存在则直接覆盖value。
6. 插入成功后，判断实际存在的键值对数量size(数组长度)是否超过了数组最大长度，如果是，则进行扩容。

##### 负载因子为0.75的原因

负载因子是扩容机制的一个阈值,假如数组容量为16，当容量到达了0.75*16=12,就会进行扩容.

如果负载因子选择较大(1.0):

我们知道数据一开始是存在数组里的，当发生了哈希冲突时，就会在该数据节点上生出一个链表。当链表长度达到一定长度时，就被转化为红黑树。

当负载因子为1.0时，意味着，只有当数组的8个位置都全部填充时，才会进行扩容。这就带来很大问题，哈希冲突是不可避免。(极端情况下有可能某个桶一直没有节点，而某个桶的链表长度过大，查询性能会降低).

总之: 当负载因子较大时，意味着会出现较大的hash冲突，底层红黑树会变得异常复杂。对查询效率极其不利。这种情况就是牺牲了时间来保证空间利用率。

如果负载因子选择较小(0.5):

当负载因子为0.5，意味着，当数组中元素达到一半就会进行扩容，填充元素少了，hash冲突也会减少，那么底层的链表长度或者红黑树的高度就会降低，查询效率会提高。

但是，此时空间利用率大大降低，原本存储1M的数据，现在就意味着需要2M的空间。

总之: 当负载因子较大时，虽然时间效率提升了，但空间利用率也降低了。

选择负载0.75是空间利用率和时间效率折中的结果。此时空间利用率还比较高，而且避免了很多的hash冲突。使得底层的链表长度或红黑树高度还比较低，提高了查询性能。

 ##### HashMap什么时候进行扩容，为什么要扩容

当hashMap的元素个数超过数组长度*loadFactor时，会进行扩容。(如当数组长度为16，loadFactor为0.75时，元素个数超过12就会进行扩容。)数组长度扩容为原来的2倍.

当HashMap元素越来越多时，碰撞的几率越来越大，所以为了提高查询的效率，需要对HashMap进行扩容。

##### 多线程put操作引起的数据丢失

比如有两个线程A和B，A希望插入一个key-value对到hashmap中。首先计算记录所要落到桶的索引坐标，然后获取到该桶里面的链表头节点。此时A的时间片用完了，而线程B被得到调度，和线程A一样执行，不过线程B成功将记录插到桶里面。假设线程A插入的记录计算出来的桶索引和线程B计算出来的一样，当线程A再次被调度执行时，它依然持有过期的链表头但它对此却一无所知，还是对该位置插入了数据，这样线程B插入的记录就凭空消失了，造成了数据不一致的情况。

##### 多线程get操作(resize)的线程安全问题

resize过程，就是容量扩大为原先容量2倍，并把当前Entry[] table数组的全部元素转移到新的table中。其他transfer函数就是实现这个功能。这个transfer的过程在并发环境下会发生错误，导致数组链表中的链表形成循环链表，在后面的get操作时e = e.next操作无限循环，Infinite Loop出现。

##### ConcurrentHashMap

JDK1.7

ConcurrentHashMap的数据结构是由一个Segmengt数组和多个HashEntry组成。

##### 初始化

ConcurrentHashMap的初始化通过位运算来初始化Segment的大小

Segment的大小最大为为65536(1<<16),没有指定concurrencyLevel，Segment的大小默认为16

```java
int size =1;
while(size < concurrencyLevel) {
++a;
size <<=1;
}    
```

每一个Segment元素下的HashEntry的初始化也是通过位运算计算。HashEntry的最小容量为2.

##### put操作

Segment实现了ReentrantLock,也就具有锁的功能。当执行put方法时，会进行第一次key的Hash来定位Segment的位置，如果Segment还没初始化，使用CAS操作进行赋值，然后进行第二次Hash，找到相应的HashEntry的位置，将数据插入指定的HashEntry位置(链表的尾端)，这里通过继承了ReentrantLock的tryLock方法获取锁，如果获取成功就插入相应位置，如果已经有线程获取该Segment的锁，当前线程会以自旋的方式去继续调用tryLock方法来获取锁，超过指定次数就挂起，等待唤醒。

##### get操作

ConcurrentHashmap的get操作和HashMap类似，只是ConcurrentHashMap第一次需要经过一次hash定位到Segment的位置，然后再hash定位到指定的HashEntry，遍历该HashEntry下的链表进行对比，成功就返回，不成功就返回null

##### size操作

在计算size时，有可能还有其他线程在不断插入元素，可能导致计算出来的size和实际的size有偏差，jdk1.7对此有两种解决方案：

第一种方案他会使用不加锁的模式去尝试多次计算ConcurrentHashMap的size，最多三次，比较前后两次计算的结果，结果一致就认为当前没有元素加入，计算的结果是准确的

第二种方案是如果第一种方案不符合，他就会给每个Segment加上锁，然后计算ConcurrentHashMap的size返回



JDK1.8

数据结构使用Node数组+链表+红黑树来实现，并发控制使用Synchronized和CAS来操作。

Node是ConcurrentHashMap存储结构的基本单元，继承于HashMap的Entry，用于存储数据。

TreeNode继承于Node，但是数据结构换成了二叉树结构，它是红黑树的数据的存储结构，用于红黑树中存储数据，当链表的节点数大于8时会转换成红黑树的结构

TreeBin从字面含义中可以理解为存储树形结构的容器，而树形结构就是指TreeNode，所以TreeBin就是封装TreeNode的容器，它提供转换黑红树的一些条件和锁的控制。

##### put操作

1. 如果没有初始化就先调用initTable方法进行初始化过程
2. 如果没有hash冲突就直接CAS插入
3. 如果还在扩容操作就先进行扩容
4. 如果存在hash冲突，就加锁来保证线程安全，有两种情况，一种是链表形式就直接遍历到尾端插入，一种是红黑树就按红黑树结构插入。
5. 如果hash冲突时会形成Node链表，当链表长度大于8并且Node数组大小超过64时，会将链表结构转化为红黑树的结构。
6. 如果添加成功就调用addCount方法统计size，并且检查是否需要扩容。

##### get操作

1. 计算hash值，定位到该table索引位置，如果是首节点符合就直接返回
2. 如果遇到扩容时，会调用标志正在扩容节点的ForwadingNode的find方法，查找该节点，匹配就返回
3. 以上都不符合的话，就往下遍历节点，匹配就返回，否则最后就返回null

##### size操作

在扩容和addCount()方法进行计算

##### 总结

JDK1.8的结构和hashmap很接近，只是增加了同步操作来实现并发控制。从jdk1.7版本的ReentrantLock+Segment+HashEntry,到jdk1.8的synchronized+CAS+HashEntry+红黑树.

1. JDK1.8的实现降低锁的粒度，JDK1.7版本锁的粒度是基于Segment的，包含多个HashEntry，而JDK1.8锁的粒度就是HashEntry（首节点）

2. JDK1.8版本的数据结构变得更加简单，使得操作也更加清晰流畅，因为已经使用synchronized来进行同步，所以不需要分段锁的概念，也就不需要Segment这种数据结构了，由于粒度的降低，实现的复杂度也增加了

3. JDK1.8使用红黑树来优化链表，基于长度很长的链表的遍历是一个很漫长的过程，而红黑树的遍历效率是很快的，代替一定阈值的链表，这样形成一个最佳拍档

4. JDK1.8为什么使用内置锁synchronized来代替重入锁ReentrantLock，我觉得有以下几点

   1. 因为粒度降低了，在相对而言的低粒度加锁方式，synchronized并不比ReentrantLock差，在粗粒度加锁中ReentrantLock可能通过Condition来控制各个低粒度的边界，更加的灵活，而在低粒度中，Condition的优势就没有了
   2. JVM的开发团队从来都没有放弃synchronized，而且基于JVM的synchronized优化空间更大，使用内嵌的关键字比使用API更加自然
   3. 在大量的数据操作下，对于JVM的内存压力，基于API的ReentrantLock会开销更多的内存，虽然不是瓶颈，但是也是一个选择依据

   

   JDK1.8为什么使用内置锁synchronized来代替重入锁ReentrantLock，我觉得有以下几点

   1. 因为粒度降低了，在相对而言的低粒度加锁方式，synchronized并不比ReentrantLock差，在粗粒度加锁中ReentrantLock可能通过Condition来控制各个低粒度的边界，更加的灵活，而在低粒度中，Condition的优势就没有了
   2. JVM的开发团队从来都没有放弃synchronized，而且基于JVM的synchronized优化空间更大，使用内嵌的关键字比使用API更加自然
   3. 在大量的数据操作下，对于JVM的内存压力，基于API的ReentrantLock会开销更多的内存，虽然不是瓶颈，但是也是一个选择依据
   4. JDK1.8为什么使用内置锁synchronized来代替重入锁ReentrantLock，我觉得有以下几点

​           1）.  因为粒度降低了，在相对而言的低粒度加锁方    式，synchronized并不比ReentrantLock差，在粗粒度加锁中ReentrantLock可能通过Condition来控制各个低粒度的边界，更加的灵活，而在低粒度中，Condition的优势就没有了

​          2). JVM的开发团队从来都没有放弃synchronized，而且基于JVM的synchronized优化空间更大，使用内嵌的关键字比使用API更加自然 

​        3). 在大量的数据操作下，对于JVM的内存压力，基于API的 ReentrantLock会开销更多的内存，虽然不是瓶颈，但是也是一个选择依据











